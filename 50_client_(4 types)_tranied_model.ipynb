{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "462240da-8b3b-4480-806a-3d944c123056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import shutil\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from fair_loss import FairLoss\n",
    "from torch.nn import functional as F\n",
    "from datasets import ADULT, German, Lawschool, HealthHeritage\n",
    "from defenses import dp_defense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbbe8aac-3678-4dc6-a219-7070445207aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97d94878-6ed1-4a55-a4ab-594ad3a5de80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinReLU(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    A linear layer followed by a ReLU activation layer.\n",
    "    \"\"\"    \n",
    "    \n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(LinReLU, self).__init__()      \n",
    "        linear = nn.Linear(in_size, out_size)\n",
    "        ReLU = nn.ReLU()\n",
    "        # self.Dropout = nn.Dropout(0.25)\n",
    "        self.layers = nn.Sequential(linear, ReLU)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.layers[0].reset_parameters()\n",
    "        return self\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "class FullyConnected(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple fully connected neural network with ReLU activations.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, layout):\n",
    "\n",
    "        super(FullyConnected, self).__init__()\n",
    "        layers = [nn.Flatten()]  # does not play any role, but makes the code neater\n",
    "        prev_fc_size = input_size\n",
    "        for i, fc_size in enumerate(layout):\n",
    "            if i + 1 < len(layout):\n",
    "                layers += [LinReLU(prev_fc_size, fc_size)]\n",
    "            else:\n",
    "                layers += [nn.Linear(prev_fc_size, 1), nn.Sigmoid()]\n",
    "                # layers += [nn.Linear(prev_fc_size, fc_size)]\n",
    "            prev_fc_size = fc_size\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0453c9-171e-4ea6-b459-bcbf092fc00d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e93ec7e-845b-4e19-9ae7-b1bcc9c1e0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_codes = [\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"FL\", \"GA\",\n",
    "               \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\",\n",
    "               \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\",\n",
    "               \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\",\n",
    "               \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2786ede-f63a-48ab-81f5-4ecf0f79935b",
   "metadata": {},
   "source": [
    "# Normal training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec24da67-fb8d-48cb-be35-f8a80f372611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client_data_dir=\"50_clients_data/processed_data/\"\n",
    "\n",
    "# layout = [100, 100, 2]\n",
    "# batch_size = 32\n",
    "# num_epochs = 10  \n",
    "# input_dim = 10\n",
    "\n",
    "# model = FullyConnected(input_dim, layout)\n",
    "# criterion = nn.BCELoss() \n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001) \n",
    "\n",
    "# for state_code in state_codes:\n",
    "    \n",
    "#     # print(state_code)    \n",
    "#     state_name=state_code\n",
    "    \n",
    "#     with open(client_data_dir+f'{state_name}.pkl', 'rb') as f:\n",
    "#         train_data_all_client  = pickle.load(f)\n",
    "    \n",
    "#     with open(client_data_dir+f'{state_name}_test.pkl', 'rb') as f:\n",
    "#         test_data  = pickle.load(f)\n",
    "\n",
    "#     print(f\"data points_{state_code}\", len(train_data_all_client)*batch_size)    \n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         running_loss = 0.0\n",
    "#         correct = 0\n",
    "#         total = 0\n",
    "    \n",
    "#         for inputs, labels in train_data_all_client:\n",
    "#             labels=labels.unsqueeze(1).float()\n",
    "#             optimizer.zero_grad()\n",
    "            \n",
    "#             outputs = model(inputs)\n",
    "            \n",
    "#             loss = criterion(outputs, labels)\n",
    "            \n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             running_loss += loss.item()\n",
    "#             predicted_classes = (outputs > 0.5).float()\n",
    "#             correct += (predicted_classes == labels).sum().item()\n",
    "#             total += labels.size(0)\n",
    "               \n",
    "#         epoch_loss = running_loss / len(train_data_all_client)\n",
    "#         accuracy = correct / total\n",
    "        \n",
    "#         # print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss:.4f}, Training Accuracy: {accuracy:.4f}\")\n",
    "       \n",
    "#         model.eval()\n",
    "#         with torch.no_grad(): \n",
    "#             val_running_loss = 0.0\n",
    "#             val_correct = 0\n",
    "#             val_total = 0\n",
    "            \n",
    "#             for inputs, labels in test_data:\n",
    "#                 labels=labels.unsqueeze(1).float()\n",
    "#                 outputs = model(inputs)\n",
    "#                 val_loss = criterion(outputs, labels)\n",
    "#                 val_running_loss += val_loss.item()\n",
    "\n",
    "#                 predicted_classes = (outputs > 0.5).float()\n",
    "#                 val_correct += (predicted_classes == labels).sum().item()\n",
    "#                 val_total += labels.size(0)\n",
    "\n",
    "                \n",
    "#                 # _, val_predicted = torch.max(outputs, 1)\n",
    "#                 # val_total += labels.size(0)\n",
    "#                 # val_correct += (val_predicted == labels).sum().item()\n",
    "        \n",
    "#         val_epoch_loss = val_running_loss / len(test_data)\n",
    "#         val_accuracy = val_correct / val_total\n",
    "#         model.train()\n",
    "        \n",
    "#     print(f\"Validation Loss: {val_epoch_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "#     model_path = f\"50_clients_data/clients_trained_model/{state_name}.pth\"\n",
    "#     torch.save(model.state_dict(), model_path)\n",
    "#     print(f\"Model saved to {model_path}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e38ebf4-3a36-4fde-ac73-936b587f8046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a5e91fc-8c6c-484b-bbe2-785d04fb0f1c",
   "metadata": {},
   "source": [
    "# Training DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daa99389-f391-44fc-8b2a-363a5f68f3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = FullyConnected(input_dim, layout)\n",
    "# criterion = nn.BCELoss() \n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf936071-27a6-4de4-90c0-2bbca149bb09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ab11a66-7644-489e-b88d-635ddb5ed2dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data points_AL 17824\n",
      "Validation Loss: 0.4283, Validation Accuracy: 0.7941\n",
      "Model saved to 50_clients_data/client_DP_trained_model/AL.pth\n",
      "\n",
      "data points_AK 2848\n",
      "Validation Loss: 0.4762, Validation Accuracy: 0.7630\n",
      "Model saved to 50_clients_data/client_DP_trained_model/AK.pth\n",
      "\n",
      "data points_AZ 26624\n",
      "Validation Loss: 0.4333, Validation Accuracy: 0.7949\n",
      "Model saved to 50_clients_data/client_DP_trained_model/AZ.pth\n",
      "\n",
      "data points_AR 11168\n",
      "Validation Loss: 0.4218, Validation Accuracy: 0.8025\n",
      "Model saved to 50_clients_data/client_DP_trained_model/AR.pth\n",
      "\n",
      "data points_CA 156544\n",
      "Validation Loss: 0.4251, Validation Accuracy: 0.8013\n",
      "Model saved to 50_clients_data/client_DP_trained_model/CA.pth\n",
      "\n",
      "data points_CO 25056\n",
      "Validation Loss: 0.4659, Validation Accuracy: 0.7745\n",
      "Model saved to 50_clients_data/client_DP_trained_model/CO.pth\n",
      "\n",
      "data points_CT 15840\n",
      "Validation Loss: 0.4437, Validation Accuracy: 0.7915\n",
      "Model saved to 50_clients_data/client_DP_trained_model/CT.pth\n",
      "\n",
      "data points_DE 3776\n",
      "Validation Loss: 0.4800, Validation Accuracy: 0.7781\n",
      "Model saved to 50_clients_data/client_DP_trained_model/DE.pth\n",
      "\n",
      "data points_FL 79168\n",
      "Validation Loss: 0.4459, Validation Accuracy: 0.7807\n",
      "Model saved to 50_clients_data/client_DP_trained_model/FL.pth\n",
      "\n",
      "data points_GA 40736\n",
      "Validation Loss: 0.4207, Validation Accuracy: 0.8040\n",
      "Model saved to 50_clients_data/client_DP_trained_model/GA.pth\n",
      "\n",
      "data points_HI 6208\n",
      "Validation Loss: 0.5020, Validation Accuracy: 0.7523\n",
      "Model saved to 50_clients_data/client_DP_trained_model/HI.pth\n",
      "\n",
      "data points_ID 6624\n",
      "Validation Loss: 0.4243, Validation Accuracy: 0.7857\n",
      "Model saved to 50_clients_data/client_DP_trained_model/ID.pth\n",
      "\n",
      "data points_IL 53632\n",
      "Validation Loss: 0.4488, Validation Accuracy: 0.7792\n",
      "Model saved to 50_clients_data/client_DP_trained_model/IL.pth\n",
      "\n",
      "data points_IN 28032\n",
      "Validation Loss: 0.4271, Validation Accuracy: 0.7975\n",
      "Model saved to 50_clients_data/client_DP_trained_model/IN.pth\n",
      "\n",
      "data points_IA 14208\n",
      "Validation Loss: 0.4293, Validation Accuracy: 0.7900\n",
      "Model saved to 50_clients_data/client_DP_trained_model/IA.pth\n",
      "\n",
      "data points_KS 12672\n",
      "Validation Loss: 0.4455, Validation Accuracy: 0.7782\n",
      "Model saved to 50_clients_data/client_DP_trained_model/KS.pth\n",
      "\n",
      "data points_KY 17632\n",
      "Validation Loss: 0.4366, Validation Accuracy: 0.7964\n",
      "Model saved to 50_clients_data/client_DP_trained_model/KY.pth\n",
      "\n",
      "data points_LA 16544\n",
      "Validation Loss: 0.4559, Validation Accuracy: 0.7772\n",
      "Model saved to 50_clients_data/client_DP_trained_model/LA.pth\n",
      "\n",
      "data points_ME 5632\n",
      "Validation Loss: 0.4375, Validation Accuracy: 0.7864\n",
      "Model saved to 50_clients_data/client_DP_trained_model/ME.pth\n",
      "\n",
      "data points_MD 26464\n",
      "Validation Loss: 0.4449, Validation Accuracy: 0.7859\n",
      "Model saved to 50_clients_data/client_DP_trained_model/MD.pth\n",
      "\n",
      "data points_MA 32096\n",
      "Validation Loss: 0.4384, Validation Accuracy: 0.7908\n",
      "Model saved to 50_clients_data/client_DP_trained_model/MA.pth\n",
      "\n",
      "data points_MI 40032\n",
      "Validation Loss: 0.4247, Validation Accuracy: 0.7991\n",
      "Model saved to 50_clients_data/client_DP_trained_model/MI.pth\n",
      "\n",
      "data points_MN 24832\n",
      "Validation Loss: 0.4552, Validation Accuracy: 0.7745\n",
      "Model saved to 50_clients_data/client_DP_trained_model/MN.pth\n",
      "\n",
      "data points_MS 10560\n",
      "Validation Loss: 0.4497, Validation Accuracy: 0.7789\n",
      "Model saved to 50_clients_data/client_DP_trained_model/MS.pth\n",
      "\n",
      "data points_MO 25344\n",
      "Validation Loss: 0.4322, Validation Accuracy: 0.7895\n",
      "Model saved to 50_clients_data/client_DP_trained_model/MO.pth\n",
      "\n",
      "data points_MT 4384\n",
      "Validation Loss: 0.4819, Validation Accuracy: 0.7418\n",
      "Model saved to 50_clients_data/client_DP_trained_model/MT.pth\n",
      "\n",
      "data points_NE 8640\n",
      "Validation Loss: 0.4876, Validation Accuracy: 0.7509\n",
      "Model saved to 50_clients_data/client_DP_trained_model/NE.pth\n",
      "\n",
      "data points_NV 11872\n",
      "Validation Loss: 0.4572, Validation Accuracy: 0.7876\n",
      "Model saved to 50_clients_data/client_DP_trained_model/NV.pth\n",
      "\n",
      "data points_NH 6400\n",
      "Validation Loss: 0.4350, Validation Accuracy: 0.7903\n",
      "Model saved to 50_clients_data/client_DP_trained_model/NH.pth\n",
      "\n",
      "data points_NJ 38240\n",
      "Validation Loss: 0.4450, Validation Accuracy: 0.7899\n",
      "Model saved to 50_clients_data/client_DP_trained_model/NJ.pth\n",
      "\n",
      "data points_NM 6976\n",
      "Validation Loss: 0.4244, Validation Accuracy: 0.7951\n",
      "Model saved to 50_clients_data/client_DP_trained_model/NM.pth\n",
      "\n",
      "data points_NY 82432\n",
      "Validation Loss: 0.4550, Validation Accuracy: 0.7777\n",
      "Model saved to 50_clients_data/client_DP_trained_model/NY.pth\n",
      "\n",
      "data points_NC 41664\n",
      "Validation Loss: 0.4413, Validation Accuracy: 0.7904\n",
      "Model saved to 50_clients_data/client_DP_trained_model/NC.pth\n",
      "\n",
      "data points_ND 3584\n",
      "Validation Loss: 0.4940, Validation Accuracy: 0.7416\n",
      "Model saved to 50_clients_data/client_DP_trained_model/ND.pth\n",
      "\n",
      "data points_OH 49728\n",
      "Validation Loss: 0.4260, Validation Accuracy: 0.7956\n",
      "Model saved to 50_clients_data/client_DP_trained_model/OH.pth\n",
      "\n",
      "data points_OK 14336\n",
      "Validation Loss: 0.4315, Validation Accuracy: 0.7982\n",
      "Model saved to 50_clients_data/client_DP_trained_model/OK.pth\n",
      "\n",
      "data points_OR 17536\n",
      "Validation Loss: 0.4489, Validation Accuracy: 0.7842\n",
      "Model saved to 50_clients_data/client_DP_trained_model/OR.pth\n",
      "\n",
      "data points_PA 54656\n",
      "Validation Loss: 0.4409, Validation Accuracy: 0.7851\n",
      "Model saved to 50_clients_data/client_DP_trained_model/PA.pth\n",
      "\n",
      "data points_RI 4576\n",
      "Validation Loss: 0.4263, Validation Accuracy: 0.8109\n",
      "Model saved to 50_clients_data/client_DP_trained_model/RI.pth\n",
      "\n",
      "data points_SC 19904\n",
      "Validation Loss: 0.4372, Validation Accuracy: 0.7813\n",
      "Model saved to 50_clients_data/client_DP_trained_model/SC.pth\n",
      "\n",
      "data points_SD 3936\n",
      "Validation Loss: 0.4597, Validation Accuracy: 0.7681\n",
      "Model saved to 50_clients_data/client_DP_trained_model/SD.pth\n",
      "\n",
      "data points_TN 27232\n",
      "Validation Loss: 0.4195, Validation Accuracy: 0.8019\n",
      "Model saved to 50_clients_data/client_DP_trained_model/TN.pth\n",
      "\n",
      "data points_TX 108768\n",
      "Validation Loss: 0.4300, Validation Accuracy: 0.7940\n",
      "Model saved to 50_clients_data/client_DP_trained_model/TX.pth\n",
      "\n",
      "data points_UT 13088\n",
      "Validation Loss: 0.3875, Validation Accuracy: 0.8219\n",
      "Model saved to 50_clients_data/client_DP_trained_model/UT.pth\n",
      "\n",
      "data points_VT 3040\n",
      "Validation Loss: 0.4710, Validation Accuracy: 0.7902\n",
      "Model saved to 50_clients_data/client_DP_trained_model/VT.pth\n",
      "\n",
      "data points_VA 36928\n",
      "Validation Loss: 0.4300, Validation Accuracy: 0.7928\n",
      "Model saved to 50_clients_data/client_DP_trained_model/VA.pth\n",
      "\n",
      "data points_WA 31968\n",
      "Validation Loss: 0.4565, Validation Accuracy: 0.7775\n",
      "Model saved to 50_clients_data/client_DP_trained_model/WA.pth\n",
      "\n",
      "data points_WV 6496\n",
      "Validation Loss: 0.4415, Validation Accuracy: 0.7809\n",
      "Model saved to 50_clients_data/client_DP_trained_model/WV.pth\n",
      "\n",
      "data points_WI 26176\n",
      "Validation Loss: 0.4606, Validation Accuracy: 0.7705\n",
      "Model saved to 50_clients_data/client_DP_trained_model/WI.pth\n",
      "\n",
      "data points_WY 2464\n",
      "Validation Loss: 0.4859, Validation Accuracy: 0.7614\n",
      "Model saved to 50_clients_data/client_DP_trained_model/WY.pth\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k=0\n",
    "client_data_dir=\"50_clients_data/processed_data/\"\n",
    "\n",
    "layout = [100, 100, 2]\n",
    "batch_size = 32\n",
    "num_epochs = 1\n",
    "input_dim = 10\n",
    "lr=0.001\n",
    "noise_scale =0.1\n",
    "\n",
    "for state_code in state_codes:\n",
    "    # if k==1:\n",
    "    #     break\n",
    "    # print(state_code)    \n",
    "    state_name=state_code\n",
    "    \n",
    "    model = FullyConnected(input_dim, layout)\n",
    "    criterion = nn.BCELoss() \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001) \n",
    "    \n",
    "    with open(client_data_dir+f'{state_name}.pkl', 'rb') as f:\n",
    "        train_data_all_client  = pickle.load(f)\n",
    "    \n",
    "    with open(client_data_dir+f'{state_name}_test.pkl', 'rb') as f:\n",
    "        test_data  = pickle.load(f)\n",
    "\n",
    "    print(f\"data points_{state_code}\", len(train_data_all_client)*batch_size)    \n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, labels in train_data_all_client:  \n",
    "\n",
    "            # print(\"inputs:: \",inputs[:2])\n",
    "            # print(\"labels:: \",labels[:2])\n",
    "            \n",
    "            permutation_indices = np.random.permutation(len(inputs))\n",
    "            X_train_permuted, y_train_permuted = inputs[permutation_indices].detach().clone(), labels[permutation_indices].detach().clone()\n",
    "\n",
    "            # print(\"X_train_permuted:: \",X_train_permuted[:2])\n",
    "            # print(\"y_train_permuted:: \",y_train_permuted[:2])\n",
    "            \n",
    "            labels=labels.unsqueeze(1).float()\n",
    "            y_train_permuted=y_train_permuted.unsqueeze(1).float()\n",
    "\n",
    "            # print(y_train_permuted)\n",
    "            # print(labels)\n",
    "            \n",
    "            # optimizer.zero_grad()\n",
    "            model.zero_grad()\n",
    "            \n",
    "            outputs = model(X_train_permuted)\n",
    "            loss = criterion(outputs, y_train_permuted)\n",
    "                        \n",
    "            grad = [g.detach() for g in torch.autograd.grad(loss, model.parameters(),retain_graph=True)]\n",
    "            # print(\"grad\",grad[0][:2])\n",
    "            \n",
    "            perturbed_grad = dp_defense(grad, noise_scale) if noise_scale > 0 else grad\n",
    "            # print(\"perturbed_grad\",perturbed_grad[0][:2])\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for p, g in zip(model.parameters(), perturbed_grad):\n",
    "                    p.data = p.data - lr * g\n",
    "                               \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            predicted_classes = (outputs > 0.5).float()\n",
    "            correct += (predicted_classes == y_train_permuted).sum().item()\n",
    "            total += y_train_permuted.size(0)\n",
    "               \n",
    "        epoch_loss = running_loss / len(train_data_all_client)\n",
    "        accuracy = correct / total\n",
    "        \n",
    "        # print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss:.4f}, Training Accuracy: {accuracy:.4f}\")\n",
    "        # k+=1\n",
    "        model.eval()\n",
    "        with torch.no_grad(): \n",
    "            val_running_loss = 0.0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            \n",
    "            for inputs, labels in test_data:\n",
    "                labels=labels.unsqueeze(1).float()\n",
    "                outputs = model(inputs)\n",
    "                val_loss = criterion(outputs, labels)\n",
    "                val_running_loss += val_loss.item()\n",
    "\n",
    "                predicted_classes = (outputs > 0.5).float()\n",
    "                val_correct += (predicted_classes == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "           \n",
    "        \n",
    "        val_epoch_loss = val_running_loss / len(test_data)\n",
    "        val_accuracy = val_correct / val_total\n",
    "        model.train()\n",
    "        \n",
    "    print(f\"Validation Loss: {val_epoch_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "    model_path = f\"50_clients_data/client_DP_trained_model/{state_name}.pth\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Model saved to {model_path}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82972cc8-ba06-4fee-8334-182d0efe35f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2e1141-f554-4d91-a919-ca2a1616879a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db92028-ff9d-431c-b9a0-e10337b3c6dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2eb7daad-9dd9-441c-ba3e-a486430b3cab",
   "metadata": {},
   "source": [
    "# Training with Fair Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75c126e7-79c1-46ed-9426-fb7b272fe825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://vi.le.gitlab.io/fair-loss/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "70e5d22c-f9ac-4a41-82e1-041ad48b686b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_iter = iter(train_data_all_client)\n",
    "# batch = next(data_iter)\n",
    "# inputs, labels = batch\n",
    "# print(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6134b3f3-33e6-4291-b77b-e32b2f32b2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.9571,  1.0448])\n"
     ]
    }
   ],
   "source": [
    "#Gender\n",
    "# print(inputs[:, 8].detach().unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b47fa7dc-fe9e-4af9-986c-2417955d8757",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_data_dir=\"50_clients_data/processed_data/\"\n",
    "\n",
    "layout = [100, 100, 2]\n",
    "batch_size = 32\n",
    "num_epochs = 10  \n",
    "input_dim = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d75a059-bfbf-4d0d-a508-dcfe29e95c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_codes = [\"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\",\n",
    "#                \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\",\n",
    "#                \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\",\n",
    "#                \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e51b30f3-1419-4cfd-b5a3-28f4b97b2ac0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data points_AL 17824\n",
      "Training Epoch [10/10], Training Loss: 0.8103, Training Accuracy: 0.8078\n",
      "Validation Loss: 0.8219, Validation Accuracy: 0.8037\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/AL.pth\n",
      "\n",
      "data points_AK 2848\n",
      "Training Epoch [10/10], Training Loss: 0.8587, Training Accuracy: 0.7948\n",
      "Validation Loss: 0.8838, Validation Accuracy: 0.7757\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/AK.pth\n",
      "\n",
      "data points_AZ 26624\n",
      "Training Epoch [10/10], Training Loss: 0.8229, Training Accuracy: 0.8040\n",
      "Validation Loss: 0.8389, Validation Accuracy: 0.8044\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/AZ.pth\n",
      "\n",
      "data points_AR 11168\n",
      "Training Epoch [10/10], Training Loss: 0.7921, Training Accuracy: 0.8122\n",
      "Validation Loss: 0.8747, Validation Accuracy: 0.8108\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/AR.pth\n",
      "\n",
      "data points_CA 156544\n",
      "Training Epoch [10/10], Training Loss: 0.8114, Training Accuracy: 0.8106\n",
      "Validation Loss: 0.8165, Validation Accuracy: 0.8114\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/CA.pth\n",
      "\n",
      "data points_CO 25056\n",
      "Training Epoch [10/10], Training Loss: 0.8907, Training Accuracy: 0.7797\n",
      "Validation Loss: 0.9047, Validation Accuracy: 0.7772\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/CO.pth\n",
      "\n",
      "data points_CT 15840\n",
      "Training Epoch [10/10], Training Loss: 0.8298, Training Accuracy: 0.8034\n",
      "Validation Loss: 0.8561, Validation Accuracy: 0.7965\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/CT.pth\n",
      "\n",
      "data points_DE 3776\n",
      "Training Epoch [10/10], Training Loss: 0.8505, Training Accuracy: 0.7981\n",
      "Validation Loss: 0.9233, Validation Accuracy: 0.7803\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/DE.pth\n",
      "\n",
      "data points_FL 79168\n",
      "Training Epoch [10/10], Training Loss: 0.8380, Training Accuracy: 0.8002\n",
      "Validation Loss: 0.8746, Validation Accuracy: 0.7882\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/FL.pth\n",
      "\n",
      "data points_GA 40736\n",
      "Training Epoch [10/10], Training Loss: 0.8212, Training Accuracy: 0.8055\n",
      "Validation Loss: 0.8257, Validation Accuracy: 0.8049\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/GA.pth\n",
      "\n",
      "data points_HI 6208\n",
      "Training Epoch [10/10], Training Loss: 0.9067, Training Accuracy: 0.7799\n",
      "Validation Loss: 0.9675, Validation Accuracy: 0.7671\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/HI.pth\n",
      "\n",
      "data points_ID 6624\n",
      "Training Epoch [10/10], Training Loss: 0.8065, Training Accuracy: 0.8060\n",
      "Validation Loss: 0.8399, Validation Accuracy: 0.7972\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/ID.pth\n",
      "\n",
      "data points_IL 53632\n",
      "Training Epoch [10/10], Training Loss: 0.8455, Training Accuracy: 0.7961\n",
      "Validation Loss: 0.8723, Validation Accuracy: 0.7860\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/IL.pth\n",
      "\n",
      "data points_IN 28032\n",
      "Training Epoch [10/10], Training Loss: 0.8142, Training Accuracy: 0.8024\n",
      "Validation Loss: 0.8232, Validation Accuracy: 0.8055\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/IN.pth\n",
      "\n",
      "data points_IA 14208\n",
      "Training Epoch [10/10], Training Loss: 0.8339, Training Accuracy: 0.7980\n",
      "Validation Loss: 0.8570, Validation Accuracy: 0.7940\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/IA.pth\n",
      "\n",
      "data points_KS 12672\n",
      "Training Epoch [10/10], Training Loss: 0.8140, Training Accuracy: 0.8022\n",
      "Validation Loss: 0.8669, Validation Accuracy: 0.7817\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/KS.pth\n",
      "\n",
      "data points_KY 17632\n",
      "Training Epoch [10/10], Training Loss: 0.8193, Training Accuracy: 0.8032\n",
      "Validation Loss: 0.8410, Validation Accuracy: 0.8055\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/KY.pth\n",
      "\n",
      "data points_LA 16544\n",
      "Training Epoch [10/10], Training Loss: 0.8447, Training Accuracy: 0.7923\n",
      "Validation Loss: 0.8912, Validation Accuracy: 0.7847\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/LA.pth\n",
      "\n",
      "data points_ME 5632\n",
      "Training Epoch [10/10], Training Loss: 0.8412, Training Accuracy: 0.7918\n",
      "Validation Loss: 0.8584, Validation Accuracy: 0.7871\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/ME.pth\n",
      "\n",
      "data points_MD 26464\n",
      "Training Epoch [10/10], Training Loss: 0.8169, Training Accuracy: 0.8062\n",
      "Validation Loss: 0.8661, Validation Accuracy: 0.7887\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/MD.pth\n",
      "\n",
      "data points_MA 32096\n",
      "Training Epoch [10/10], Training Loss: 0.8222, Training Accuracy: 0.8056\n",
      "Validation Loss: 0.8487, Validation Accuracy: 0.7962\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/MA.pth\n",
      "\n",
      "data points_MI 40032\n",
      "Training Epoch [10/10], Training Loss: 0.8176, Training Accuracy: 0.8055\n",
      "Validation Loss: 0.8253, Validation Accuracy: 0.8034\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/MI.pth\n",
      "\n",
      "data points_MN 24832\n",
      "Training Epoch [10/10], Training Loss: 0.8882, Training Accuracy: 0.7831\n",
      "Validation Loss: 0.8975, Validation Accuracy: 0.7772\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/MN.pth\n",
      "\n",
      "data points_MS 10560\n",
      "Training Epoch [10/10], Training Loss: 0.7868, Training Accuracy: 0.8161\n",
      "Validation Loss: 0.8755, Validation Accuracy: 0.7911\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/MS.pth\n",
      "\n",
      "data points_MO 25344\n",
      "Training Epoch [10/10], Training Loss: 0.8242, Training Accuracy: 0.8005\n",
      "Validation Loss: 0.8399, Validation Accuracy: 0.7961\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/MO.pth\n",
      "\n",
      "data points_MT 4384\n",
      "Training Epoch [10/10], Training Loss: 0.8737, Training Accuracy: 0.7815\n",
      "Validation Loss: 0.9486, Validation Accuracy: 0.7582\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/MT.pth\n",
      "\n",
      "data points_NE 8640\n",
      "Training Epoch [10/10], Training Loss: 0.8511, Training Accuracy: 0.7904\n",
      "Validation Loss: 0.9233, Validation Accuracy: 0.7639\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/NE.pth\n",
      "\n",
      "data points_NV 11872\n",
      "Training Epoch [10/10], Training Loss: 0.8763, Training Accuracy: 0.7899\n",
      "Validation Loss: 0.9034, Validation Accuracy: 0.7859\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/NV.pth\n",
      "\n",
      "data points_NH 6400\n",
      "Training Epoch [10/10], Training Loss: 0.8649, Training Accuracy: 0.7867\n",
      "Validation Loss: 0.8461, Validation Accuracy: 0.7991\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/NH.pth\n",
      "\n",
      "data points_NJ 38240\n",
      "Training Epoch [10/10], Training Loss: 0.8458, Training Accuracy: 0.7995\n",
      "Validation Loss: 0.8353, Validation Accuracy: 0.8045\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/NJ.pth\n",
      "\n",
      "data points_NM 6976\n",
      "Training Epoch [10/10], Training Loss: 0.7976, Training Accuracy: 0.8156\n",
      "Validation Loss: 0.8108, Validation Accuracy: 0.8065\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/NM.pth\n",
      "\n",
      "data points_NY 82432\n",
      "Training Epoch [10/10], Training Loss: 0.8777, Training Accuracy: 0.7865\n",
      "Validation Loss: 0.8865, Validation Accuracy: 0.7848\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/NY.pth\n",
      "\n",
      "data points_NC 41664\n",
      "Training Epoch [10/10], Training Loss: 0.8109, Training Accuracy: 0.8072\n",
      "Validation Loss: 0.8307, Validation Accuracy: 0.8031\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/NC.pth\n",
      "\n",
      "data points_ND 3584\n",
      "Training Epoch [10/10], Training Loss: 0.9127, Training Accuracy: 0.7722\n",
      "Validation Loss: 0.9576, Validation Accuracy: 0.7438\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/ND.pth\n",
      "\n",
      "data points_OH 49728\n",
      "Training Epoch [10/10], Training Loss: 0.8134, Training Accuracy: 0.8071\n",
      "Validation Loss: 0.8368, Validation Accuracy: 0.7991\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/OH.pth\n",
      "\n",
      "data points_OK 14336\n",
      "Training Epoch [10/10], Training Loss: 0.8091, Training Accuracy: 0.8067\n",
      "Validation Loss: 0.8529, Validation Accuracy: 0.7937\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/OK.pth\n",
      "\n",
      "data points_OR 17536\n",
      "Training Epoch [10/10], Training Loss: 0.8460, Training Accuracy: 0.7955\n",
      "Validation Loss: 0.8765, Validation Accuracy: 0.7894\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/OR.pth\n",
      "\n",
      "data points_PA 54656\n",
      "Training Epoch [10/10], Training Loss: 0.8416, Training Accuracy: 0.7976\n",
      "Validation Loss: 0.8672, Validation Accuracy: 0.7895\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/PA.pth\n",
      "\n",
      "data points_RI 4576\n",
      "Training Epoch [10/10], Training Loss: 0.8595, Training Accuracy: 0.7910\n",
      "Validation Loss: 0.8356, Validation Accuracy: 0.8021\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/RI.pth\n",
      "\n",
      "data points_SC 19904\n",
      "Training Epoch [10/10], Training Loss: 0.7998, Training Accuracy: 0.8091\n",
      "Validation Loss: 0.8568, Validation Accuracy: 0.7944\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/SC.pth\n",
      "\n",
      "data points_SD 3936\n",
      "Training Epoch [10/10], Training Loss: 0.8132, Training Accuracy: 0.8102\n",
      "Validation Loss: 0.9334, Validation Accuracy: 0.7630\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/SD.pth\n",
      "\n",
      "data points_TN 27232\n",
      "Training Epoch [10/10], Training Loss: 0.8159, Training Accuracy: 0.8040\n",
      "Validation Loss: 0.8148, Validation Accuracy: 0.8104\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/TN.pth\n",
      "\n",
      "data points_TX 108768\n",
      "Training Epoch [10/10], Training Loss: 0.8290, Training Accuracy: 0.8018\n",
      "Validation Loss: 0.8419, Validation Accuracy: 0.7981\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/TX.pth\n",
      "\n",
      "data points_UT 13088\n",
      "Training Epoch [10/10], Training Loss: 0.7293, Training Accuracy: 0.8274\n",
      "Validation Loss: 0.7502, Validation Accuracy: 0.8203\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/UT.pth\n",
      "\n",
      "data points_VT 3040\n",
      "Training Epoch [10/10], Training Loss: 0.8963, Training Accuracy: 0.7859\n",
      "Validation Loss: 0.8967, Validation Accuracy: 0.7782\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/VT.pth\n",
      "\n",
      "data points_VA 36928\n",
      "Training Epoch [10/10], Training Loss: 0.8152, Training Accuracy: 0.8085\n",
      "Validation Loss: 0.8308, Validation Accuracy: 0.7995\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/VA.pth\n",
      "\n",
      "data points_WA 31968\n",
      "Training Epoch [10/10], Training Loss: 0.8594, Training Accuracy: 0.7949\n",
      "Validation Loss: 0.8955, Validation Accuracy: 0.7882\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/WA.pth\n",
      "\n",
      "data points_WV 6496\n",
      "Training Epoch [10/10], Training Loss: 0.8324, Training Accuracy: 0.7917\n",
      "Validation Loss: 0.8582, Validation Accuracy: 0.7907\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/WV.pth\n",
      "\n",
      "data points_WI 26176\n",
      "Training Epoch [10/10], Training Loss: 0.8603, Training Accuracy: 0.7888\n",
      "Validation Loss: 0.8917, Validation Accuracy: 0.7796\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/WI.pth\n",
      "\n",
      "data points_WY 2464\n",
      "Training Epoch [10/10], Training Loss: 0.8757, Training Accuracy: 0.7846\n",
      "Validation Loss: 0.9852, Validation Accuracy: 0.7631\n",
      "Model saved to 50_clients_data/clients_fair_trained_model/WY.pth\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = FullyConnected(input_dim, layout)\n",
    "criterion = nn.BCELoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) \n",
    "\n",
    "for state_code in state_codes:\n",
    "    \n",
    "    # print(state_code)    \n",
    "    state_name=state_code\n",
    "\n",
    "    model = FullyConnected(input_dim, layout)\n",
    "    criterion = nn.BCELoss() \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001) \n",
    "    \n",
    "    with open(client_data_dir+f'{state_name}.pkl', 'rb') as f:\n",
    "        train_data_all_client  = pickle.load(f)\n",
    "    \n",
    "    with open(client_data_dir+f'{state_name}_test.pkl', 'rb') as f:\n",
    "        test_data  = pickle.load(f)\n",
    "\n",
    "    print(f\"data points_{state_code}\", len(train_data_all_client)*batch_size)    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "    \n",
    "        for inputs, labels in train_data_all_client:\n",
    "            fair_loss=FairLoss(torch.nn.BCELoss(), inputs[:, 8].detach().unique(), 'accuracy')\n",
    "            # print(inputs[:, 8].detach().unique())\n",
    "            \n",
    "            labels=labels.unsqueeze(1).float()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            loss_1 = criterion(outputs, labels)\n",
    "            loss_2 = fair_loss(inputs[:, 8],outputs,labels)              \n",
    "            final_loss=loss_1+loss_2\n",
    "            final_loss.backward()\n",
    "            \n",
    "            optimizer.step()            \n",
    "            running_loss += final_loss.item()\n",
    "            predicted_classes = (outputs > 0.5).float()\n",
    "            correct += (predicted_classes == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "        # print(loss2,\"and\",final_loss) \n",
    "        \n",
    "        epoch_loss = running_loss / len(train_data_all_client)\n",
    "        accuracy = correct / total\n",
    "        \n",
    "        # print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss:.4f}, Training Accuracy: {accuracy:.4f}\")\n",
    "       \n",
    "        model.eval()\n",
    "        with torch.no_grad(): \n",
    "            val_running_loss = 0.0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            \n",
    "            for inputs, labels in test_data:\n",
    "                fair_loss=FairLoss(torch.nn.BCELoss(), inputs[:, 8].detach().unique(), 'accuracy')\n",
    "                \n",
    "                labels=labels.unsqueeze(1).float()\n",
    "                outputs = model(inputs)\n",
    "                val_loss1 = criterion(outputs, labels)\n",
    "                val_loss2 = fair_loss(inputs[:, 8],outputs,labels) \n",
    "                val_final_loss=val_loss1+val_loss2\n",
    "                \n",
    "                val_running_loss += val_final_loss.item()\n",
    "\n",
    "                predicted_classes = (outputs > 0.5).float()\n",
    "                val_correct += (predicted_classes == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        \n",
    "        val_epoch_loss = val_running_loss / len(test_data)\n",
    "        val_accuracy = val_correct / val_total\n",
    "        model.train()\n",
    "        \n",
    "    # print(f\"Training Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss:.4f}, Training Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Validation Loss: {val_epoch_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")   \n",
    "    \n",
    "    model_path = f\"50_clients_data/clients_fair_trained_model/{state_name}.pth\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Model saved to {model_path}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d5ef71-a21c-4f28-8642-cc4b4c0520ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7385e18c-d837-4a70-b613-c270e88cadb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7dd79a-a64f-473b-bbb5-4f91a6635a75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96716b57-56c1-43bf-91ae-147efae21a35",
   "metadata": {},
   "source": [
    "# DP and Fairness: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60b59feb-f0c3-4144-bff4-97e0c01b9aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_data_dir=\"50_clients_data/processed_data/\"\n",
    "\n",
    "layout = [100, 100, 2]\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "input_dim = 10\n",
    "lr=0.001\n",
    "noise_scale =0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe922ca6-5d96-4ebc-8061-c495a7e60844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4db1b8b-520e-4392-a732-ff9fd4aceea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_codes = [\"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\",\n",
    "#                \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\",\n",
    "#                \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\",\n",
    "#                \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acdb511e-2452-46b4-b50a-35f510daf854",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data points_AL 17824\n",
      "Validation Loss: 0.8294, Validation Accuracy: 0.8028\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/AL.pth\n",
      "\n",
      "data points_AK 2848\n",
      "Validation Loss: 0.8874, Validation Accuracy: 0.7842\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/AK.pth\n",
      "\n",
      "data points_AZ 26624\n",
      "Validation Loss: 0.8357, Validation Accuracy: 0.8021\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/AZ.pth\n",
      "\n",
      "data points_AR 11168\n",
      "Validation Loss: 0.8249, Validation Accuracy: 0.8176\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/AR.pth\n",
      "\n",
      "data points_CA 156544\n",
      "Validation Loss: 0.8165, Validation Accuracy: 0.8113\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/CA.pth\n",
      "\n",
      "data points_CO 25056\n",
      "Validation Loss: 0.9146, Validation Accuracy: 0.7777\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/CO.pth\n",
      "\n",
      "data points_CT 15840\n",
      "Validation Loss: 0.8694, Validation Accuracy: 0.7955\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/CT.pth\n",
      "\n",
      "data points_DE 3776\n",
      "Validation Loss: 0.9367, Validation Accuracy: 0.7749\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/DE.pth\n",
      "\n",
      "data points_FL 79168\n",
      "Validation Loss: 0.8786, Validation Accuracy: 0.7885\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/FL.pth\n",
      "\n",
      "data points_GA 40736\n",
      "Validation Loss: 0.8232, Validation Accuracy: 0.8081\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/GA.pth\n",
      "\n",
      "data points_HI 6208\n",
      "Validation Loss: 0.9592, Validation Accuracy: 0.7646\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/HI.pth\n",
      "\n",
      "data points_ID 6624\n",
      "Validation Loss: 0.8375, Validation Accuracy: 0.8002\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/ID.pth\n",
      "\n",
      "data points_IL 53632\n",
      "Validation Loss: 0.8778, Validation Accuracy: 0.7839\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/IL.pth\n",
      "\n",
      "data points_IN 28032\n",
      "Validation Loss: 0.8217, Validation Accuracy: 0.8044\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/IN.pth\n",
      "\n",
      "data points_IA 14208\n",
      "Validation Loss: 0.8486, Validation Accuracy: 0.7965\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/IA.pth\n",
      "\n",
      "data points_KS 12672\n",
      "Validation Loss: 0.8710, Validation Accuracy: 0.7792\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/KS.pth\n",
      "\n",
      "data points_KY 17632\n",
      "Validation Loss: 0.8455, Validation Accuracy: 0.8048\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/KY.pth\n",
      "\n",
      "data points_LA 16544\n",
      "Validation Loss: 0.8822, Validation Accuracy: 0.7820\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/LA.pth\n",
      "\n",
      "data points_ME 5632\n",
      "Validation Loss: 0.8657, Validation Accuracy: 0.7886\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/ME.pth\n",
      "\n",
      "data points_MD 26464\n",
      "Validation Loss: 0.8630, Validation Accuracy: 0.7910\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/MD.pth\n",
      "\n",
      "data points_MA 32096\n",
      "Validation Loss: 0.8466, Validation Accuracy: 0.8004\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/MA.pth\n",
      "\n",
      "data points_MI 40032\n",
      "Validation Loss: 0.8446, Validation Accuracy: 0.8020\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/MI.pth\n",
      "\n",
      "data points_MN 24832\n",
      "Validation Loss: 0.9128, Validation Accuracy: 0.7756\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/MN.pth\n",
      "\n",
      "data points_MS 10560\n",
      "Validation Loss: 0.8542, Validation Accuracy: 0.7948\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/MS.pth\n",
      "\n",
      "data points_MO 25344\n",
      "Validation Loss: 0.8443, Validation Accuracy: 0.7972\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/MO.pth\n",
      "\n",
      "data points_MT 4384\n",
      "Validation Loss: 0.9279, Validation Accuracy: 0.7582\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/MT.pth\n",
      "\n",
      "data points_NE 8640\n",
      "Validation Loss: 0.9358, Validation Accuracy: 0.7681\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/NE.pth\n",
      "\n",
      "data points_NV 11872\n",
      "Validation Loss: 0.9110, Validation Accuracy: 0.7832\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/NV.pth\n",
      "\n",
      "data points_NH 6400\n",
      "Validation Loss: 0.8476, Validation Accuracy: 0.7991\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/NH.pth\n",
      "\n",
      "data points_NJ 38240\n",
      "Validation Loss: 0.8385, Validation Accuracy: 0.8037\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/NJ.pth\n",
      "\n",
      "data points_NM 6976\n",
      "Validation Loss: 0.8177, Validation Accuracy: 0.8037\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/NM.pth\n",
      "\n",
      "data points_NY 82432\n",
      "Validation Loss: 0.8847, Validation Accuracy: 0.7849\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/NY.pth\n",
      "\n",
      "data points_NC 41664\n",
      "Validation Loss: 0.8378, Validation Accuracy: 0.7985\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/NC.pth\n",
      "\n",
      "data points_ND 3584\n",
      "Validation Loss: 0.9627, Validation Accuracy: 0.7461\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/ND.pth\n",
      "\n",
      "data points_OH 49728\n",
      "Validation Loss: 0.8327, Validation Accuracy: 0.8000\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/OH.pth\n",
      "\n",
      "data points_OK 14336\n",
      "Validation Loss: 0.8482, Validation Accuracy: 0.8002\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/OK.pth\n",
      "\n",
      "data points_OR 17536\n",
      "Validation Loss: 0.8903, Validation Accuracy: 0.7883\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/OR.pth\n",
      "\n",
      "data points_PA 54656\n",
      "Validation Loss: 0.8557, Validation Accuracy: 0.7939\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/PA.pth\n",
      "\n",
      "data points_RI 4576\n",
      "Validation Loss: 0.8447, Validation Accuracy: 0.7960\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/RI.pth\n",
      "\n",
      "data points_SC 19904\n",
      "Validation Loss: 0.8440, Validation Accuracy: 0.7958\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/SC.pth\n",
      "\n",
      "data points_SD 3936\n",
      "Validation Loss: 0.9231, Validation Accuracy: 0.7620\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/SD.pth\n",
      "\n",
      "data points_TN 27232\n",
      "Validation Loss: 0.8113, Validation Accuracy: 0.8076\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/TN.pth\n",
      "\n",
      "data points_TX 108768\n",
      "Validation Loss: 0.8457, Validation Accuracy: 0.7984\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/TX.pth\n",
      "\n",
      "data points_UT 13088\n",
      "Validation Loss: 0.7527, Validation Accuracy: 0.8203\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/UT.pth\n",
      "\n",
      "data points_VT 3040\n",
      "Validation Loss: 0.9187, Validation Accuracy: 0.7782\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/VT.pth\n",
      "\n",
      "data points_VA 36928\n",
      "Validation Loss: 0.8318, Validation Accuracy: 0.7981\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/VA.pth\n",
      "\n",
      "data points_WA 31968\n",
      "Validation Loss: 0.8915, Validation Accuracy: 0.7859\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/WA.pth\n",
      "\n",
      "data points_WV 6496\n",
      "Validation Loss: 0.8621, Validation Accuracy: 0.7858\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/WV.pth\n",
      "\n",
      "data points_WI 26176\n",
      "Validation Loss: 0.8963, Validation Accuracy: 0.7788\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/WI.pth\n",
      "\n",
      "data points_WY 2464\n",
      "Validation Loss: 0.9361, Validation Accuracy: 0.7647\n",
      "Model saved to 50_clients_data/clients_DP_Fair_trained_model/WY.pth\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# k=0\n",
    "\n",
    "for state_code in state_codes:\n",
    "    # if k==1:\n",
    "    #     break\n",
    "    # print(state_code)    \n",
    "    state_name=state_code\n",
    "        \n",
    "    model = FullyConnected(input_dim, layout)\n",
    "    criterion = nn.BCELoss() \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001) \n",
    "    \n",
    "    with open(client_data_dir+f'{state_name}.pkl', 'rb') as f:\n",
    "        train_data_all_client  = pickle.load(f)\n",
    "    \n",
    "    with open(client_data_dir+f'{state_name}_test.pkl', 'rb') as f:\n",
    "        test_data  = pickle.load(f)\n",
    "\n",
    "    print(f\"data points_{state_code}\", len(train_data_all_client)*batch_size)    \n",
    "        \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, labels in train_data_all_client:  \n",
    "            \n",
    "            permutation_indices = np.random.permutation(len(inputs))\n",
    "            X_train_permuted, y_train_permuted = inputs[permutation_indices].detach().clone(), labels[permutation_indices].detach().clone()\n",
    "\n",
    "            fair_loss=FairLoss(torch.nn.BCELoss(), X_train_permuted[:, 8].detach().unique(), 'accuracy')\n",
    "            \n",
    "            labels=labels.unsqueeze(1).float()\n",
    "            y_train_permuted=y_train_permuted.unsqueeze(1).float()\n",
    "\n",
    "            # print(y_train_permuted)\n",
    "            # print(labels)\n",
    "            \n",
    "            # optimizer.zero_grad()\n",
    "            model.zero_grad()\n",
    "            \n",
    "            outputs = model(X_train_permuted)\n",
    "            # loss = criterion(outputs, y_train_permuted)\n",
    "\n",
    "            loss_1 = criterion(outputs, y_train_permuted)\n",
    "            loss_2 = fair_loss(X_train_permuted[:, 8],outputs,y_train_permuted)              \n",
    "            final_loss=loss_1+loss_2\n",
    "            \n",
    "            grad = [g.detach() for g in torch.autograd.grad(final_loss, model.parameters(),retain_graph=True)]\n",
    "            \n",
    "            perturbed_grad = dp_defense(grad, noise_scale) if noise_scale > 0 else grad\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for p, g in zip(model.parameters(), perturbed_grad):\n",
    "                    p.data = p.data - lr * g\n",
    "                               \n",
    "            final_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += final_loss.item()\n",
    "            predicted_classes = (outputs > 0.5).float()\n",
    "            correct += (predicted_classes == y_train_permuted).sum().item()\n",
    "            total += y_train_permuted.size(0)\n",
    "               \n",
    "        epoch_loss = running_loss / len(train_data_all_client)\n",
    "        accuracy = correct / total\n",
    "        \n",
    "        # print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss:.4f}, Training Accuracy: {accuracy:.4f}\")\n",
    "        # k+=1\n",
    "        model.eval()\n",
    "        with torch.no_grad(): \n",
    "            val_running_loss = 0.0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            \n",
    "            for inputs, labels in test_data:\n",
    "                fair_loss=FairLoss(torch.nn.BCELoss(), inputs[:, 8].detach().unique(), 'accuracy')\n",
    "                \n",
    "                labels=labels.unsqueeze(1).float()\n",
    "                outputs = model(inputs)\n",
    "                val_loss1 = criterion(outputs, labels)\n",
    "                val_loss2 = fair_loss(inputs[:, 8],outputs,labels) \n",
    "                \n",
    "                val_final_loss = val_loss1+val_loss2\n",
    "                \n",
    "                val_running_loss += val_final_loss.item()\n",
    "\n",
    "                predicted_classes = (outputs > 0.5).float()\n",
    "                val_correct += (predicted_classes == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "           \n",
    "        \n",
    "        val_epoch_loss = val_running_loss / len(test_data)\n",
    "        val_accuracy = val_correct / val_total\n",
    "        model.train()\n",
    "        \n",
    "    # print(f\"Training Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss:.4f}, Training Accuracy: {accuracy:.4f}\") \n",
    "    print(f\"Validation Loss: {val_epoch_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "    model_path = f\"50_clients_data/clients_DP_Fair_trained_model/{state_name}.pth\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Model saved to {model_path}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b8f88f-37df-457b-81ca-2201b4febadd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e309328a-55c6-4d19-b423-b27298eff472",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e82b57-8a85-4fef-a3be-f1961cd9e777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190c0419-ede8-4e8a-ad9c-530d94d29248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cdf87c-2988-472f-a206-aff0c7a7617d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82c453d-a61c-4a26-afa9-a33cd283147c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9bb012-b5e9-49f3-977d-c8f4bb529e58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a8e32fe-ba1a-465a-9021-cd624b2784e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_name=\"AL\"\n",
    "\n",
    "# client_data_dir=\"50_clients_data/processed_data/\"\n",
    "\n",
    "# with open(client_data_dir+f'{state_name}.pkl', 'rb') as f:\n",
    "#     train_data_all_client  = pickle.load(f)\n",
    "\n",
    "# with open(client_data_dir+f'{state_name}_test.pkl', 'rb') as f:\n",
    "#     test_data  = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f047e2e8-fc80-4388-a5d1-98d9d9352e72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b419ee3c-3073-40a4-8be8-be2c876f5064",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8396b36c-dabb-454e-898f-51a1830690cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2dcd94-6b45-4c8f-b254-ecc5acf56ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462a918b-63c2-455d-b918-65369f4b4a77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e84ff4-462d-4fd6-a166-40c93d124898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "132941e3-b83b-4e8f-ae43-b1b712c49fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 1., 2., 0., 2., 1., 0., 4., 0., 2.], grad_fn=<SelectBackward>) tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<ReluBackward0>) tensor([[2.],\n",
      "        [1.],\n",
      "        [4.],\n",
      "        [3.],\n",
      "        [0.],\n",
      "        [4.],\n",
      "        [3.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [1.]])\n",
      "torch.Size([10]) torch.Size([10, 1]) torch.Size([10, 1])\n",
      "tensor([[1., 2., 2., 3., 3.],\n",
      "        [3., 1., 0., 2., 0.],\n",
      "        [4., 2., 3., 2., 3.],\n",
      "        [3., 0., 3., 2., 1.],\n",
      "        [3., 2., 4., 4., 0.],\n",
      "        [4., 1., 1., 3., 1.],\n",
      "        [1., 0., 2., 0., 1.],\n",
      "        [4., 4., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 4.],\n",
      "        [3., 2., 1., 1., 0.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from fair_loss import FairLoss\n",
    "\n",
    "model = torch.nn.Sequential(torch.nn.Linear(5, 1), torch.nn.ReLU())\n",
    "data = torch.randint(0, 5, (10, 5), dtype=torch.float, requires_grad=True)\n",
    "\n",
    "y_true = torch.randint(0, 5, (10, 1), dtype=torch.float)\n",
    "y_pred = model(data)\n",
    "\n",
    "# Let's say the sensitive attribute is in the second dimension\n",
    "dim = 1\n",
    "criterion = FairLoss(torch.nn.BCELoss(), data[:, dim].detach().unique(), 'accuracy')\n",
    "\n",
    "print(data[:, dim], y_pred, y_true)\n",
    "print(data[:, dim].shape, y_pred.shape, y_true.shape)\n",
    "\n",
    "loss = criterion(data[:, dim], y_pred, y_true)\n",
    "\n",
    "# loss = criterion(data[:, dim], y_pred.unsqueeze(1), y_true.unsqueeze(1))\n",
    "\n",
    "loss.backward()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3614051-f60b-4ec3-8268-02e05175d6ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m output\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_conda/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_conda/lib/python3.8/site-packages/torch/nn/modules/loss.py:961\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 961\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_conda/lib/python3.8/site-packages/torch/nn/functional.py:2468\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2467\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 2468\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnll_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_conda/lib/python3.8/site-packages/torch/nn/functional.py:2264\u001b[0m, in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected input batch_size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) to match target batch_size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2262\u001b[0m                      \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), target\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)))\n\u001b[1;32m   2263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m-> 2264\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnll_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m   2266\u001b[0m     ret \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mnll_loss2d(\u001b[38;5;28minput\u001b[39m, target, weight, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), ignore_index)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "# Example of target with class indices\n",
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "# Example of target with class probabilities\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5).softmax(dim=1)\n",
    "output = loss(input, target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1200ef-49cd-40d8-a9b5-0c94da199e05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b455260-79c6-42d4-ae57-6f3bf2ca38a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b0a962-a7ff-47ec-9c6b-bfa51d6eb27a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3866b1-843c-43c0-b033-52fc37334272",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cce1448-65aa-4349-aed9-e1263381d9eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0f1536-5a9f-47fe-ae60-649de3a046da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "833a8304-7135-45d6-bc04-153aa174df3b",
   "metadata": {},
   "source": [
    "#  Testing one client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f76bbdf-c9d6-4b75-81b8-295bc85de309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_data_dir=\"50_clients_data/processed_data/\"\n",
    "\n",
    "with open(client_data_dir+'AL_test.pkl', 'rb') as f:\n",
    "    test_data  = pickle.load(f)\n",
    "    \n",
    "model =  FullyConnected(input_dim, layout)\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fba8bd0-9994-43a0-9fa0-53b8f03b9d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82588242-e09f-4ec1-8e4d-18070cff5f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.4091, Test Accuracy: 0.8040\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "test_loss = 0.0\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    for inputs, labels in test_data:\n",
    "        # Move data to the same device as model\n",
    "        # inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        # print(outputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Calculate average loss and accuracy\n",
    "average_test_loss = test_loss / len(test_data)\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f\"Test Loss: {average_test_loss:.4f}, Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd74cb4f-73af-463d-8ab0-86cc41f48a11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edb5032-752c-44a3-b68e-4f5ad410a134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f42387e-6118-46aa-9f56-22140203eedc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13ff7f3-9c88-449c-b7c6-95214e847f3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c954893-dc29-48b9-b48a-8cf073993629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf4ff9e-8740-4e77-9f91-469535298d49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1007ab-99c8-4fcc-8b5c-0b78e1db0db7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbc36b4-20b3-4cea-9f4f-5ab5c937094f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8349c5e6-3e18-49ee-9ad0-e79f6917f64f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a60b1b0-9b15-4bf2-8da4-86d4832dad04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e539f3-09ea-43bf-b5d7-f4f570bf36e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "af603ccf-2d0b-4bc7-8af7-be29cf09ad1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: AL, df Length: 8907\n",
      "State: AK, df Length: 3546\n",
      "State: AZ, df Length: 13311\n",
      "State: AR, df Length: 6964\n",
      "State: CA, df Length: 19566\n",
      "State: CO, df Length: 12522\n",
      "State: CT, df Length: 9892\n",
      "State: DE, df Length: 4713\n",
      "State: FL, df Length: 19785\n",
      "State: GA, df Length: 15274\n",
      "State: HI, df Length: 7731\n",
      "State: ID, df Length: 4132\n",
      "State: IL, df Length: 20105\n",
      "State: IN, df Length: 14009\n",
      "State: IA, df Length: 8872\n",
      "State: KS, df Length: 7904\n",
      "State: KY, df Length: 8802\n",
      "State: LA, df Length: 8267\n",
      "State: ME, df Length: 7002\n",
      "State: MD, df Length: 13217\n",
      "State: MA, df Length: 12034\n",
      "State: MI, df Length: 15002\n",
      "State: MN, df Length: 12408\n",
      "State: MS, df Length: 6594\n",
      "State: MO, df Length: 12666\n",
      "State: MT, df Length: 5463\n",
      "State: NE, df Length: 5392\n",
      "State: NV, df Length: 7404\n",
      "State: NH, df Length: 7966\n",
      "State: NJ, df Length: 14334\n",
      "State: NM, df Length: 4356\n",
      "State: NY, df Length: 10302\n",
      "State: NC, df Length: 15620\n",
      "State: ND, df Length: 4455\n",
      "State: OH, df Length: 18640\n",
      "State: OK, df Length: 8958\n",
      "State: OR, df Length: 8768\n",
      "State: PA, df Length: 20492\n",
      "State: RI, df Length: 5712\n",
      "State: SC, df Length: 9952\n",
      "State: SD, df Length: 4899\n",
      "State: TN, df Length: 13601\n",
      "State: TX, df Length: 13592\n",
      "State: UT, df Length: 8168\n",
      "State: VT, df Length: 3767\n",
      "State: VA, df Length: 13843\n",
      "State: WA, df Length: 15978\n",
      "State: WV, df Length: 4052\n",
      "State: WI, df Length: 13076\n",
      "State: WY, df Length: 3064\n"
     ]
    }
   ],
   "source": [
    "merge_dfs={}\n",
    "for state_code, (features, label) in dfs.items():\n",
    "\n",
    "\n",
    "    merge_df = pd.concat([features, label], axis=1)\n",
    "    merge_df = merge_df.dropna()\n",
    "\n",
    "    if 8000 <len(merge_df) < 20000:\n",
    "        merge_df_sampled = merge_df.sample(frac=0.5, random_state=42)\n",
    "\n",
    "    elif 20000 <len(merge_df) < 40000:\n",
    "        merge_df_sampled = merge_df.sample(frac=0.4, random_state=42)\n",
    "    \n",
    "    elif 40000 <len(merge_df) < 70000:        \n",
    "         merge_df_sampled = merge_df.sample(frac=0.3, random_state=42)\n",
    "        \n",
    "    elif 70000<len(merge_df)<99000:       \n",
    "        merge_df_sampled = merge_df.sample(frac=0.2, random_state=42)\n",
    "\n",
    "    elif len(merge_df)>100000:      \n",
    "        merge_df_sampled = merge_df.sample(frac=0.1, random_state=42)\n",
    "    else:\n",
    "        merge_df_sampled = merge_df\n",
    "    \n",
    "    # merge_df_sampled = merge_df.sample(frac=0.2, random_state=42)\n",
    "    merge_df_sampled['PINCP'] = merge_df_sampled['PINCP'].replace({True: '>50K', False: '<=50K'})\n",
    "\n",
    "    merge_dfs[state_code] = merge_df_sampled\n",
    "\n",
    "for state_code, df in merge_dfs.items():\n",
    "    print(f\"State: {state_code}, df Length: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288ca799-fc57-4cd5-87ca-546f7d19bc47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d223db8-45dc-473e-a9c8-25466aa142ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a93aad86-252d-4a8c-a4f6-661ca48862e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for state_code, (features, label) in dfs.items():\n",
    "#     # take 30%\n",
    "#     num_rows_to_keep = int(len(features) * 0.3) \n",
    "#     random_indices = np.random.choice(len(features), num_rows_to_keep, replace=False)\n",
    "#     reduced_features = features.iloc[random_indices]\n",
    "#     reduced_label = label.iloc[random_indices]\n",
    "#     dfs[state_code] = (reduced_features, reduced_label)\n",
    "\n",
    "# for state_code, (reduced_features, reduced_label) in dfs.items():\n",
    "#     print(f\"State: {state_code}, Reduced Features Length: {len(reduced_features)}, Reduced Label Length: {len(reduced_label)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a55148d4-3246-4808-ad56-98a288027ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the dictionary to a file\n",
    "with open('dfs.pickle', 'wb') as f:\n",
    "    pickle.dump(merge_dfs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c36daf13-b0c9-443d-893f-48b468e56723",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dfs.pickle', 'rb') as f:\n",
    "    dfs_loaded = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "869ef307-92e7-481b-98a0-bb2f0f8e4a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: AL, df Length: 4454\n",
      "State: AK, df Length: 709\n",
      "State: AZ, df Length: 6655\n",
      "State: AR, df Length: 2786\n",
      "State: CA, df Length: 39133\n",
      "State: CO, df Length: 6261\n",
      "State: CT, df Length: 3957\n",
      "State: DE, df Length: 943\n",
      "State: FL, df Length: 19785\n",
      "State: GA, df Length: 10183\n",
      "State: HI, df Length: 1546\n",
      "State: ID, df Length: 1653\n",
      "State: IL, df Length: 13403\n",
      "State: IN, df Length: 7004\n",
      "State: IA, df Length: 3549\n",
      "State: KS, df Length: 3161\n",
      "State: KY, df Length: 4401\n",
      "State: LA, df Length: 4133\n",
      "State: ME, df Length: 1400\n",
      "State: MD, df Length: 6608\n",
      "State: MA, df Length: 8023\n",
      "State: MI, df Length: 10002\n",
      "State: MN, df Length: 6204\n",
      "State: MS, df Length: 2638\n",
      "State: MO, df Length: 6333\n",
      "State: MT, df Length: 1093\n",
      "State: NE, df Length: 2157\n",
      "State: NV, df Length: 2961\n",
      "State: NH, df Length: 1593\n",
      "State: NJ, df Length: 9556\n",
      "State: NM, df Length: 1742\n",
      "State: NY, df Length: 20604\n",
      "State: NC, df Length: 10413\n",
      "State: ND, df Length: 891\n",
      "State: OH, df Length: 12427\n",
      "State: OK, df Length: 3583\n",
      "State: OR, df Length: 4384\n",
      "State: PA, df Length: 13662\n",
      "State: RI, df Length: 1142\n",
      "State: SC, df Length: 4976\n",
      "State: SD, df Length: 980\n",
      "State: TN, df Length: 6801\n",
      "State: TX, df Length: 27185\n",
      "State: UT, df Length: 3267\n",
      "State: VT, df Length: 753\n",
      "State: VA, df Length: 9229\n",
      "State: WA, df Length: 7989\n",
      "State: WV, df Length: 1621\n",
      "State: WI, df Length: 6538\n",
      "State: WY, df Length: 613\n"
     ]
    }
   ],
   "source": [
    "for state_code, df in dfs_loaded.items():\n",
    "    print(f\"State: {state_code}, df Length: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a7a2d099-4666-42e0-8243-db5888c33c3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGEP</th>\n",
       "      <th>COW</th>\n",
       "      <th>SCHL</th>\n",
       "      <th>MAR</th>\n",
       "      <th>OCCP</th>\n",
       "      <th>POBP</th>\n",
       "      <th>RELP</th>\n",
       "      <th>WKHP</th>\n",
       "      <th>SEX</th>\n",
       "      <th>RAC1P</th>\n",
       "      <th>PINCP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5420.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2320.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4710.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>76.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5940.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27180</th>\n",
       "      <td>60.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9130.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27181</th>\n",
       "      <td>74.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4435.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27182</th>\n",
       "      <td>43.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4720.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27183</th>\n",
       "      <td>32.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5410.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27184</th>\n",
       "      <td>26.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2545.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27185 rows  11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       AGEP  COW  SCHL  MAR    OCCP  POBP  RELP  WKHP  SEX  RAC1P  PINCP\n",
       "0      20.0  2.0  16.0  5.0  5420.0  48.0   0.0  12.0  2.0    1.0  <=50K\n",
       "1      27.0  3.0  19.0  1.0  2320.0  18.0   0.0  40.0  2.0    1.0  <=50K\n",
       "2      56.0  1.0  19.0  5.0  4710.0  48.0  15.0  50.0  1.0    1.0   >50K\n",
       "3      27.0  1.0  21.0  5.0  4000.0  48.0   0.0  38.0  2.0    1.0  <=50K\n",
       "4      76.0  3.0  21.0  1.0  5940.0  35.0   1.0  13.0  1.0    1.0  <=50K\n",
       "...     ...  ...   ...  ...     ...   ...   ...   ...  ...    ...    ...\n",
       "27180  60.0  1.0  16.0  1.0  9130.0  48.0   0.0  40.0  1.0    1.0  <=50K\n",
       "27181  74.0  1.0  21.0  1.0  4435.0   6.0   0.0  45.0  1.0    1.0   >50K\n",
       "27182  43.0  1.0  16.0  5.0  4720.0  48.0   0.0  40.0  2.0    1.0  <=50K\n",
       "27183  32.0  1.0  19.0  5.0  5410.0   6.0   0.0  40.0  1.0    1.0   >50K\n",
       "27184  26.0  3.0  21.0  5.0  2545.0  48.0   2.0  38.0  2.0    1.0  <=50K\n",
       "\n",
       "[27185 rows x 11 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_loaded[\"TX\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067e7e84-6d73-4379-abdb-89054d2141a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
